{
  "cells": [
    {
      "metadata": {
        "_uuid": "64d5b29ca37add902c1176746baa2c510dfc7baf"
      },
      "cell_type": "markdown",
      "source": "Import library"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import tensor\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lrSheduler\nimport torchvision\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\n\nimport pandas as pd\nimport torch as th\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nimport scipy as sci",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e7841e23197a16d90ad16df1e2126d6bea20e14d"
      },
      "cell_type": "markdown",
      "source": "Load data from csv"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "10e205460ec2bbb79e9e60de29ffa7748d580d7a"
      },
      "cell_type": "code",
      "source": "dataRawTrain = pd.read_csv('../input/fashion-mnist_train.csv')\ndataRawTest = pd.read_csv('../input/fashion-mnist_test.csv')\n\ndataTrain = np.array(dataRawTrain.iloc[:,1:]).astype('uint8')\ndataTrain = dataTrain.reshape(-1, 1, 28, 28)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3fa1bee7898639c35359a40e5c5461b2e18b8588"
      },
      "cell_type": "markdown",
      "source": "**Generate target output**\n\n2 -> [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n\n6 -> [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n\n..."
    },
    {
      "metadata": {
        "_uuid": "fe60a555149a9dd652cbf0d000651068219f4b70",
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "targetTrain = np.array(dataRawTrain.iloc[:,:1])\nzz = np.zeros([len(targetTrain), 10])\nfor i, d in enumerate(targetTrain):\n    zz[i][d.squeeze()] = 0.66\ntargetTrain = zz\n\ndataTest = np.array(dataRawTest.iloc[:,1:])\ndataTest = dataTest.reshape(-1, 1, 28, 28)\n\ntargetTest = np.array(dataRawTest.iloc[:,:1])\nzz = np.zeros([len(targetTest), 10])\nfor i, d in enumerate(targetTest):\n    zz[i][d.squeeze()] = 0.66\ntargetTest = zz\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cf93da67ce45e7abfb79e36aff07d714babc684a"
      },
      "cell_type": "markdown",
      "source": "Show database"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "17d5fd30c5bf614c103d014204d8b7ca42e32712"
      },
      "cell_type": "code",
      "source": "for i in range(3):\n    plt.matshow(dataTrain[i][0], cmap='rainbow')\n    plt.colorbar()\n    plt.suptitle('class: {}'.format(targetTrain[i]))\n    \nfor i in range(3):    \n    plt.matshow(dataTest[i][0], cmap='rainbow')\n    plt.colorbar()\n    plt.suptitle('class: {}'.format(targetTest[i]))\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a24874ee5e4a6d1ae73749e1f50176cd1cc52216",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "Data normalisation and create pyTorch data loader"
    },
    {
      "metadata": {
        "_uuid": "8436734456824587c4d886a495f87243f10d1253",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from torch.utils.data import Dataset, DataLoader, TensorDataset\nfrom torchvision import transforms\n\ndataTrainT = torch.from_numpy(dataTrain).float() / np.std(dataTrain) - np.mean(dataTrain)\n# dataTrainT = torch.from_numpy(dataTrain).float() / 255.0 - 0.5\ntargetTrainT = torch.from_numpy(targetTrain).float()\nprint('train data size: {}'.format(dataTrainT.shape))\n\ndataTestT = torch.from_numpy(dataTest).float() / np.std(dataTest) - np.mean(dataTest)\n# dataTestT = torch.from_numpy(dataTest).float() /  255.0 - 0.5\ntargetTestT = torch.from_numpy(targetTest).float()\nprint('test data size: {}'.format(dataTestT.shape))\n\n# processing function for trained data\ndef processImgBatch(batch):\n    outBatchData = []\n    outBatchTarget = []\n    for i, (img, target) in enumerate(batch):\n        outBatchData.append(img)\n        outBatchTarget.append(target)\n    outTensorData = th.stack(outBatchData)\n    outTensorTarget = th.stack(outBatchTarget)\n    return [outTensorData, outTensorTarget]\n\ndataTrainArch = TensorDataset(dataTrainT, targetTrainT)\nloaderTrain = DataLoader(dataTrainArch, shuffle=True, batch_size=64, pin_memory=True, collate_fn=processImgBatch)\n\ndataTestArch = TensorDataset(dataTestT, targetTestT)\nloaderTest = DataLoader(dataTestArch, shuffle=False, batch_size=1000, pin_memory=True, collate_fn=processImgBatch)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e362397815781b854257c1d06c147c13d9bb9c0e"
      },
      "cell_type": "markdown",
      "source": "Network"
    },
    {
      "metadata": {
        "_uuid": "93d0299a9b4267fa98fa4b6b2d5cb223c3fefdd6",
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "class NetMNIST2(nn.Module):\n    flag = False\n\n    def __init__(self):\n        super(NetMNIST2, self).__init__()\n        \n        # convolution 1\n        self.bn0 = nn.BatchNorm2d(1)\n        self.conv00 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1, bias=False)\n        self.relu00 = nn.PReLU()\n        self.conv0 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias=False)\n        self.relu0 = nn.PReLU()\n        self.pool0 = nn.MaxPool2d(2)\n        self.drop0 = nn.Dropout2d()\n        \n        # convolution 2\n        self.bn1 = nn.BatchNorm2d(32)\n        self.conv10 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.relu10 = nn.PReLU()\n        self.conv1 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.relu1 = nn.PReLU()\n        self.pool1 = nn.MaxPool2d(2)\n        self.drop1 = nn.Dropout2d()\n\n        # convolution 3\n        self.bn2 = nn.BatchNorm2d(64)\n        self.conv20 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=False)\n        self.relu20 = nn.PReLU()\n        self.conv2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False)\n        self.relu2 = nn.PReLU()\n        self.pool2 = nn.MaxPool2d(2)\n        self.drop2 = nn.Dropout2d()\n\n        # full connection layer 1\n        self.bn3 = nn.BatchNorm2d(128)\n        self.fc1 = nn.Linear(32*6*6, 10)\n        self.fc1Prelu = nn.PReLU()\n        \n        # full connection layer 2\n        self.fc2 = nn.Linear(10, 10)\n        \n    # direct computation\n    def forward(self, x):\n        x = self.bn0(x)\n        x = self.conv00(x)\n        x = self.relu00(x)\n        x = self.conv0(x)\n        x = self.relu0(x)\n        x = self.pool0(x)\n        x = self.drop0(x)\n\n        x = self.bn1(x)\n        x = self.conv10(x)\n        x = self.relu10(x)\n        x = self.conv1(x)\n        x = self.relu1(x)\n        x = self.pool1(x)\n        x = self.drop1(x)\n\n        x = self.bn2(x)\n        x = self.conv20(x)\n        x = self.relu20(x)\n        x = self.conv2(x)\n        x = self.relu2(x)\n        x = self.pool2(x)\n        x = self.drop2(x)\n        \n#         show input parameter\n#         print(x.shape)\n        x = self.bn3(x)\n        x = self.fc1(x.view(x.size(0), -1))\n        x = self.fc1Prelu(x)\n        x = torch.nn.functional.dropout(x)\n        x = self.fc2(x)\n        x = F.softmax(x, dim=1)\n        return x\n\n# network initialization\nnet = NetMNIST2().cuda()\n# show network\nprint(net)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "169d31606f5547299ecb5858b500670e08ab1406"
      },
      "cell_type": "markdown",
      "source": "Train network"
    },
    {
      "metadata": {
        "_uuid": "876baefb50108da12afdf6e552d0519e1518b9c6",
        "trusted": true
      },
      "cell_type": "code",
      "source": "optimizer = optim.Adam(net.parameters(), lr=1e-4, weight_decay=1e-5)\ncriterion = F.kl_div\n\ndef train(model, optimizer, criterion, dataLoader):\n    model.train()\n    for i, (data, target) in enumerate(dataLoader):\n        dataCUDA, targetCUDA = Variable(data).cuda(), Variable(target.float()).cuda()\n        optimizer.zero_grad()\n        outModel = model(dataCUDA)\n        loss = criterion(outModel, targetCUDA)\n        loss.backward()\n        optimizer.step()\n    return loss\n\n\ndef test(model, dataLoader):\n    model.eval()\n    fit = 0.0\n\n    for i, (data, target) in enumerate(dataLoader):\n        dataCUDA, targetCUDA = Variable(data).cuda(), Variable(target).cuda()\n        outModel = model(dataCUDA)\n        pred = outModel.data.max(1)[1]\n        targetMaxIndex = targetCUDA.data.max(1)[1]     \n        fit += pred.eq(targetMaxIndex).cpu().sum()\n#     calculation accuracy\n    acc = float(fit.cpu().numpy() / float(len(dataLoader.dataset)))\n    return acc\n\n\nlossProgress = []\nfor e in range(5):\n    lossTrain = train(net, optimizer, criterion, loaderTrain)\n    accTest = test(net, loaderTest)\n    lossProgress.append(accTest)\n    print('Epoch: {}   acc: {}   loss: {}'.format(e, accTest, lossTrain))\n    \nplt.plot(lossProgress)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7a0525a361ce2602edba4756913710d765ffbc77"
      },
      "cell_type": "raw",
      "source": ""
    },
    {
      "metadata": {
        "_uuid": "347c873a268e73b728dce21dc0505a12d6d7178b"
      },
      "cell_type": "markdown",
      "source": "## Confusion matrix calulate"
    },
    {
      "metadata": {
        "_uuid": "bb5f2cd23f5a231d41dc479c2058893772dc1faf",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import confusion_matrix\nX = np.empty([0], dtype=np.int16)\nY = np.empty([0], dtype=np.int16)\nfor i, (data, target) in enumerate(loaderTest):\n    dataCUDA, targetCUDA = Variable(data).cuda(), Variable(target).cuda()\n    outModel = net(dataCUDA)\n    Y = np.append(Y, outModel.data.max(1)[1].cpu().numpy())\n    X = np.append(X, targetCUDA.data.max(1)[1].cpu().numpy())\n    \nfrom matplotlib.ticker import MultipleLocator\nlabels = [str(x) for x in range(10)]\nconfMatrix = confusion_matrix(X.reshape(-1), Y.reshape(-1))\n\nimport seaborn as sns\nax= plt.subplot()\nsns.heatmap(confMatrix, annot=True, fmt='d', ax = ax, cmap='plasma')\n\nax.set_xlabel('Predicted')\nax.set_ylabel('Target')\nax.set_title('Confusion Matrix')\nax.xaxis.set_ticklabels(labels)\nax.yaxis.set_ticklabels(labels)\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a8b4a3dcbf77d1865906844f71b6835dc8d34d23"
      },
      "cell_type": "markdown",
      "source": "## Precision recall  accuracy calculate"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cf53f36c17db9636f0e473813988a3c599cfa75e"
      },
      "cell_type": "code",
      "source": "TPR0 = confMatrix[0][0]\nFPR0 = confMatrix[0][:].sum() - TPR0\nprint(FPR)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d130d3b258e29ecb552a30f4113d8edb02edab8a"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}